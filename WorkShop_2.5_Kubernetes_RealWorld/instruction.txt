Instruction for Workshop 2.5 Kubernetes in RealWorld:
Note: This instruction will start lab for kubernetes's cluster for real workshop:

0. Reinitial your machine before grouping with your friend by command:
	sudo su -
    kubeadm reset
	sudo rm -rf /var/lib/etcd
    docker system prune -af
	reboot

	*Optional: For cleansig workshop on lab machine*
	cd ~
	rm -rf kubernetes_201911/
	git clone https://github.com/praparn/kubernetes_201911.git
	
1. Check "LAB" sheet for your group and inform your team for all node information like below:

====================================================
Lab Description: (Check you excel sheet)
ClusterName: XXXXX
CA-Cert-Hash: XXXXX
Token: XXXXX
Ingress CNAME: XXXXX

Machine name		            			Roles:			IP Address: (Private)		IP Address: (Public)			Hostname
Training_DockerZerotoHero_StudentGX_1	   	Master			10.0.1.X					X.X.X.X							ip-10-0-1-X.ap-southeast-1.compute.internal
Training_DockerZerotoHero_StudentGX_2       NodePort		10.0.1.X					X.X.X.X							ip-10-0-1-X.ap-southeast-1.compute.internal
Training_DockerZerotoHero_StudentGX_3   	NodePort		10.0.1.X					X.X.X.X							ip-10-0-1-X.ap-southeast-1.compute.internal
===================================================
0. Follow document pdf for access ssh (Windows/MACOS)

1. (all node) SSH/Putty to target machine with command below:
ssh -i docker_lab ubuntu@<Public IP Address of Master>
ssh -i docker_lab ubuntu@<Public IP Address of NodePort1>
ssh -i docker_lab ubuntu@<Public IP Address of NodePort2>

2. *Optional* (all node) Setup TMUX script and SSH for Share Session:
sudo apt-get update && sudo apt-get install -y tmux
tmux new -s Lab

	# Remark: For your co-worker please kindly ssh to target node and join session with command #
		tmux attach-session -t Lab

3. *Optional* (all node) Check hostname for each node and record by command:
	curl http://169.254.169.254/latest/meta-data/local-hostname

4. (Master) Prepare configuration for initial kubernetes master
	cd ~
	sed -i -e 's/name: hostnamemaster/name: <hostname of master node>/g' ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-init.yaml
	sed -i -e 's/clusterName: Kubernetes/clusterName: <ClusterName>/g' ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-init.yaml
	sed -i -e 's/1.1.1.1/<Public IP Address of Master>/g' ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-init.yaml
	more ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-init.yaml

5. (Master) initial cluster by command:
	sudo su -
	swapoff -a
	kubeadm init --config /home/ubuntu/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-init.yaml
	exit

	*Remark: Need to record token Output
    -------------------------------------------------
    Token output:
    -------------------------------------------------
    kubeadm join 10.0.1.246:6443 --token 4hqsau.ozlonb6302xc0cn4 --discovery-token-ca-cert-hash sha256:600b3f66c2051914b9f51fcfb104b6b2e8d0a4c0d25f9d386010ef8d34dc9024

	*Remark: Collect ca-cert-hash and token
	-------------------------------------------------

6. (Master) Setup run cluster system by command (Regular User):
	mkdir -p $HOME/.kube
  	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config	==> Answer: "Y"
	sudo chown $(id -u):$(id -g) $HOME/.kube/config
	kubectl taint nodes --all node-role.kubernetes.io/master-

7. (Master) Create calico net plugin for network for cluster by command:
    kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml

8. (Master) Check master readiness and dns by command (Take 5 - 10 min):
	watch kubectl get pods --all-namespaces

========================================  Create Dashboard =====================================================================
9. (Master) Create Dashboard by command:
	kubectl create namespace kubernetes-dashboard
	kubectl create secret generic kubernetes-dashboard-certs --from-file=$HOME/kubernetes_201911/WorkShop_1.7_Resource_Management/cert -n kubernetes-dashboard
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_201911/master/WorkShop_1.7_Resource_Management/dashboard-restrict.yml
	
10. (Master) Check Bearer Token for access by command:
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_201911/master/WorkShop_1.7_Resource_Management/user-restrict.yml
	kubectl get secret -n kubernetes-dashboard 
	kubectl describe secret <admin-user-token-xxx> -n kubernetes-dashboard
	*Remark: Record Token:
	Ex:
	------------------------------------------------------------------------------------
 eyJhbGciOiJSUzI1NiIsImtpZCI6ImZ6RnRXNmI3STdLR3VRS2lLSjU0LV9wU2hySzVKS1JDdXczQ2NkTmU4eVkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTQ0Zm1nIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI3NmNjNWRiMC0yMWRlLTQyZTItOWMxNi1lYjExZDhkMTAyNmMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.Eoj6wFbMpeBbBYmnWx9IbeuHHDswsboWmtsmymFmRhAy0a8ivYs-yFYpOo1rIHte4EuKwGllIHbOIdyosWnpPXF_7q_yxhjTk1tfOPAzbyIFIzggasXJwIpTQReH7pu7cHE6dd4olJYtiklfI_UotM4VvvAGpL8xaWPRx8Am9laL11LJFccmiWaw9YjQlU1-1iMzSwVKWHkX990MGGNNr0hXSQSYPHYyjaiyU1RoSRjJyhwxXhV-nneB3qDqUjJiIVEtoAoIR0taQdrXYqxLiwdZGyfhsFJWNxlYhMGcLiLE-IH8TA_7kvTQg7pqJtkcfRiLUxmAajlrHNaiCEtPFQ
 	------------------------------------------------------------------------------------

11. (Master) Open Kubernetes's forward for operate:
	kubectl get pods -n=kubernetes-dashboard
	kubectl port-forward --address 0.0.0.0 pods/<kubernetes-dashboard-xxx> 8443:8443 -n=kubernetes-dashboard
	(Ex: kubectl port-forward --address 0.0.0.0 pods/kubernetes-dashboard-7b5bf5d559-stwjc 8443:8443 -n=kubernetes-dashboard)

	*Test by open browser: 
	https://<Public IP>:8443
========================================  Create Dashboard =====================================================================

12. (Worker1) Prepare configuration for initial kubernetes worker:
cd ~
sed -i -e 's/token: tokenid/token: <token id>/g' ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-join.yaml
sed -i -e 's/sha256:cahash/<ca-cert-hash>/g' ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-join.yaml
sed -i -e 's/apiServerEndpoint: hostnamemaster:6443/apiServerEndpoint: <full name of master node>:6443/g' ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-join.yaml
sed -i -e 's/name: hostnameworker/name: <full name of worker1 node>/g' ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-join.yaml
more ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-join.yaml

13. (Worker1) join cluster by command:
sudo su -
swapoff -a
kubeadm join --config /home/ubuntu/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-join.yaml
exit
htop

14. (Worker2) Prepare configuration for initial kubernetes worker:
cd ~
sed -i -e 's/token: tokenid/token: <token id>/g' ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-join.yaml
sed -i -e 's/sha256:cahash/<ca-cert-hash>/g' ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-join.yaml
sed -i -e 's/apiServerEndpoint: hostnamemaster:6443/apiServerEndpoint: <full name of master node>:6443/g' ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-join.yaml
sed -i -e 's/name: hostnameworker/name: <full name of worker2 node>/g' ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-join.yaml
more ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-join.yaml

15. (Worker2) join cluster by command:
sudo su -
swapoff -a
kubeadm join --config /home/ubuntu/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/kubeadm-join.yaml
exit
htop

16. (Master) Check Node in Cluster by command (This take 5 - 10 min):
watch kubectl get nodes

17. (Master)Check Pods from all cluster system running by command:
watch kubectl get pods --all-namespaces

18. (Master) Test deployment basic nginx pods by command
kubectl run webtest --image=labdocker/nginx:http2 --port=443
kubectl get pods -o wide
kubectl expose deployment webtest --target-port=443 --type=NodePort
kubectl get svc -o wide                                               ==> Record Public Port

19. (loal) Test get web outside:
https://x.x.x.x:xxxx
https://x.x.x.x:xxxx
https://x.x.x.x:xxxx

20. (Master) Cleanup Lab by command:
kubectl delete deployment/webtest
kubectl delete svc/webtest

====================================== Create Ingress Controller====================================================

21. (Master) Create ingress set:
	21.1. Create mandatory resource by command: 
	kubectl apply -f ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/ingress-nginx/deploy/static/mandatory.yaml
	*Remark: If you need to modified config. Edit this file first
	more ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/ingress-nginx/deploy/static/mandatory.yaml
========================================
Example:
[...]
kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
data:
  use-http2: "true"
  proxy-body-size: "50m"
[...]
========================================

	21.2 Check file service:
	more ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/ingress-nginx/deploy/static/provider/aws/service-l4.yaml
========================================
Example:
[...]
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
  annotations:
    # Enable PROXY protocol
    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: "*"
    # Ensure the ELB idle timeout is less than nginx keep-alive timeout. By default,
    # NGINX keep-alive is set to 75s. If using WebSockets, the value will need to be
    # increased to '3600' to avoid any potential issues.
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "60"
	service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
	service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: "2"
	service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: "2"
	service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "30"
	service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: "5"
[...]
========================================

	21.3 Apply service and configuration as below:
	kubectl apply -f ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/ingress-nginx/deploy/static/provider/aws/service-l4.yaml
	kubectl apply -f ~/kubernetes_201911/WorkShop_2.5_Kubernetes_RealWorld/ingress-nginx/deploy/static/provider/aws/patch-configmap-l4.yaml

	21.4 Check by command: 
	watch kubectl get pods -n=ingress-nginx

	21.5 Collect external cname by command:
	kubectl get svc -n=ingress-nginx	(This may take several miniute before finished) ==> Record "CNAME of ELB"
========================================
Example:
NAME            TYPE           CLUSTER-IP       EXTERNAL-IP                                                                   PORT(S)                      AGE
ingress-nginx   LoadBalancer   10.108.187.142   af4e0d29c410111e9a11002f20ffeb23-988151595.ap-southeast-1.elb.amazonaws.com   80:31252/TCP,443:31461/TCP   17s
========================================

25. (Master) Test deploy ingress service

# Create Service/Pods/Deployment for webtest1 and webtest2 by command:
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_201911/master/WorkShop_2.3_Ingress_Network/webtest_deploy.yml
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_201911/master/WorkShop_2.3_Ingress_Network/webtest_deploy2.yml

# View service for connection by command:
	kubectl get svc -o wide

# Create ingress for access by command:
	kubectl create -f https://raw.githubusercontent.com/praparn/kubernetes_201911/master/WorkShop_2.3_Ingress_Network/ingress_webtest.yml
	kubectl get ing -o wide
	kubectl describe ing/ingresswebtest

26. (Master/Local (MAC)) Test access website by command or browser:
	curl http://<CNAME AWS's ELB> -H 'Host:webtest1.kuberneteslabthailand.com'
	curl http://<CNAME AWS's ELB> -H 'Host:webtest2.kuberneteslabthailand.com'

27. (Master) Delete Ingress by command:
	kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_201911/master/WorkShop_2.3_Ingress_Network/ingress_webtest.yml
	
28. (Master) Clean Up Lab:
	kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_201911/master/WorkShop_2.3_Ingress_Network/webtest_deploy.yml
	kubectl delete -f https://raw.githubusercontent.com/praparn/kubernetes_201911/master/WorkShop_2.3_Ingress_Network/webtest_deploy2.yml